{
  "flags": {
    "data_file": {
      "path": "file_path",
      "type": "path",
      "description": "Override the dataset path resolved from the configuration file."
    },
    "output_dir": {
      "path": "base_dir",
      "type": "path",
      "description": "Override the orchestrator base directory for run artefacts."
    },
    "time_steps": {
      "path": "data.time_steps",
      "type": "int",
      "description": "Set the sliding window length passed into the DataLoader."
    },
    "epochs": {
      "path": "training.epochs",
      "type": "int",
      "description": "Set the number of training epochs."
    },
    "batch_size": {
      "path": "training.batch_size",
      "type": "int",
      "description": "Set the mini-batch size consumed by the trainer."
    },
    "filters_conv1": {
      "path": "model.filters_conv1",
      "type": "int",
      "description": "Adjust the number of filters in the first convolutional block."
    },
    "kernel_size_conv1": {
      "path": "model.kernel_size_conv1",
      "type": "int",
      "description": "Adjust the kernel width for the first convolutional block."
    },
    "l2_reg_output": {
      "path": "model.output_l2_reg",
      "type": "float",
      "description": "Set the L2 regularisation applied to the output layer."
    },
    "leaky_alpha_conv1": {
      "path": "model.leaky_relu_alpha_conv1",
      "type": "float",
      "description": "Set the LeakyReLU alpha used in the first convolution of a residual block."
    },
    "leaky_alpha_conv2": {
      "path": "model.leaky_relu_alpha_conv2",
      "type": "float",
      "description": "Set the LeakyReLU alpha used in the second convolution of a residual block."
    },
    "leaky_alpha_conv1_res_block": {
      "path": "model.leaky_relu_alpha_conv1_res_block",
      "type": "float",
      "description": "Set the LeakyReLU alpha for the first convolution inside the V6/V7 residual block."
    },
    "leaky_alpha_conv2_res_block": {
      "path": "model.leaky_relu_alpha_conv2_res_block",
      "type": "float",
      "description": "Set the LeakyReLU alpha for the second convolution inside the V6/V7 residual block."
    },
    "leaky_alpha_after_residual_add": {
      "path": "model.leaky_relu_alpha_after_residual_add",
      "type": "float",
      "description": "Set the LeakyReLU alpha applied after summing residual branches."
    },
    "leaky_alpha_after_add_res_block": {
      "path": "model.leaky_relu_alpha_after_add_res_block",
      "type": "float",
      "description": "Set the LeakyReLU alpha applied after addition in the residual block variant."
    },
    "leaky_alpha_dense_after_flatten": {
      "path": "model.leaky_relu_alpha_dense_after_flatten",
      "type": "float",
      "description": "Set the LeakyReLU alpha for the dense layer that follows flattening in V6."
    },
    "leaky_alpha_intermediate_dense": {
      "path": "model.leaky_relu_alpha_intermediate_dense",
      "type": "float",
      "description": "Set the LeakyReLU alpha for the optional intermediate dense layer in V5."
    },
    "conv_l2_reg": {
      "path": "model.conv_l2_reg",
      "type": "float",
      "description": "Set the L2 regularisation term applied to convolution kernels."
    },
    "conv_l2_reg_res_block": {
      "path": "model.conv_l2_reg_res_block",
      "type": "float",
      "description": "Set the L2 regularisation term for residual block convolutions in V6/V7."
    },
    "dense_units_after_flatten": {
      "path": "model.dense_units_after_flatten",
      "type": "int",
      "description": "Set the number of units in the dense layer after flattening (V6)."
    },
    "dense_l2_reg_after_flatten": {
      "path": "model.dense_l2_reg_after_flatten",
      "type": "float",
      "description": "Set the L2 penalty applied to the dense layer after flattening (V6)."
    },
    "intermediate_dense_units": {
      "path": "model.intermediate_dense_units",
      "type": "int",
      "description": "Set the number of units in the optional intermediate dense layer (V5)."
    },
    "key_dim_final_mha": {
      "path": "model.key_dim_final_mha",
      "type": "int",
      "description": "Set the key dimension for the final multi-head attention block (V5)."
    },
    "key_dim_residual_block": {
      "path": "model.key_dim_residual_block",
      "type": "int",
      "description": "Set the key dimension for the residual attention blocks in V3-V5."
    },
    "key_dim_res_block": {
      "path": "model.key_dim_residual_block",
      "type": "int",
      "description": "Legacy alias for V6/V7 residual block key dimension."
    },
    "lstm_units": {
      "path": "model.lstm_units",
      "type": "int",
      "description": "Set the number of units in the BiLSTM stack."
    },
    "lstm_l2_reg": {
      "path": "model.lstm_l2_reg",
      "type": "float",
      "description": "Set the L2 regularisation applied to LSTM weights."
    },
    "optimizer_lr": {
      "path": "model.optimizer_lr",
      "type": "float",
      "description": "Override the optimiser learning rate."
    },
    "recurrent_dropout_lstm": {
      "path": "model.recurrent_dropout_lstm",
      "type": "float",
      "description": "Set the recurrent dropout applied inside the BiLSTM."
    },
    "moe_leaky_relu_alpha": {
      "path": "model.moe_leaky_relu_alpha",
      "type": "float",
      "description": "Set the LeakyReLU alpha used inside the Mixture-of-Experts gating (V7+)."
    },
    "moe_num_experts": {
      "path": "model.moe_num_experts",
      "type": "int",
      "description": "Set the number of experts inside the Mixture-of-Experts block (V7+)."
    },
    "moe_units": {
      "path": "model.moe_units",
      "type": "int",
      "description": "Set the hidden units for experts inside the Mixture-of-Experts block (V7+)."
    },
    "num_bilstm_layers": {
      "path": "model.num_bilstm_layers",
      "type": "int",
      "description": "Set how many BiLSTM layers are stacked."
    },
    "num_heads_final_mha": {
      "path": "model.num_heads_final_mha",
      "type": "int",
      "description": "Set the number of heads for the final multi-head attention block (V5)."
    },
    "num_heads_residual_block": {
      "path": "model.num_heads_residual_block",
      "type": "int",
      "description": "Set the number of attention heads inside the residual attention blocks (V3-V5)."
    },
    "num_heads_res_block": {
      "path": "model.num_heads_residual_block",
      "type": "int",
      "description": "Legacy alias for the residual block head count used by V6/V7."
    },
    "output_l2_reg": {
      "path": "model.output_l2_reg",
      "type": "float",
      "description": "Alias for `l2_reg_output`."
    }
  },
  "toggles": {
    "disable_mixed_precision": {
      "path": "runtime.mixed_precision.enabled",
      "value": false,
      "description": "Force eager execution with float32 tensors."
    },
    "enable_mixed_precision": {
      "path": "runtime.mixed_precision.enabled",
      "value": true,
      "description": "Enable automatic mixed precision if the GPU supports it."
    },
    "use_intermediate_dense": {
      "path": "model.use_intermediate_dense",
      "value": true,
      "description": "Enable the optional intermediate dense layer introduced in V5."
    }
  }
}
